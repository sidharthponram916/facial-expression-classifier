# -*- coding: utf-8 -*-
"""Predict Facial Expressions with FER2013 Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kcz6Bf8xpM4ge0-I_RbCdO0DxXqQuf_6
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd;
import numpy as np;
import time;

from IPython.display import clear_output;
from sklearn.model_selection import train_test_split;
from tensorflow import keras;
from matplotlib import pyplot as plot;

ds = pd.read_csv('/content/drive/MyDrive/CSV/fer2013.csv');

ds.head()

# Index Emotions to Its Corresponding Type
emotions = {
    0: "Angry",
    1: "Disgust",
    2: "Fear",
    3: "Happy",
    4: "Sad",
    5: "Surprised",
    6: "Neutral"
}

ds.shape

# This is a count of all the expression types in the FER2013 Dataset. We know that there will be a problem with detecting disgust, since there is a lack of examples.

angry = 0;
disgust = 0;
fear = 0;
happy = 0;
sad = 0;
surprise = 0;
neutral = 0;

print("Count of Each Emotion in the FER2013 Dataset");
for i in range (0, len(ds.emotion)):
  emotion = ds.emotion.loc[i];

  if (emotion == 0):
    angry = angry + 1;
  elif (emotion == 1):
    disgust = disgust + 1;
  elif (emotion == 2):
    fear = fear + 1;
  elif (emotion == 3):
    happy = happy + 1;
  elif (emotion == 4):
    sad = sad + 1;
  elif (emotion == 5):
    surprise = surprise + 1;
  elif (emotion == 6):
    neutral = neutral + 1;

print(f"Anger: {angry}")
print(f"Disgust: {disgust} ");
print(f"Fear: {fear} ");
print(f"Happy: {happy} ");
print(f"Sad: {sad} ");
print(f"Surprised: {surprise} ");
print(f"Neutral: {neutral} ");
print(f"Total of {angry + disgust + fear + happy + sad + surprise + neutral} Images.")

disgust = 0;
fear = 0;
happy = 0;
sad = 0;
surprise = 0;
neutral = 0;

# Display all the images in a Dynamic Way

for i in range(0, len(ds.pixels)):
  image = np.array(ds.pixels.loc[i].split(' ')).reshape(48,48).astype(float);
  plot.xlabel("Emotion - " + emotions[ds.emotion.loc[i]]);
  plot.imshow(image);
  plot.show();
  time.sleep(2);
  clear_output(wait=True)

# Reshaped all values within the dataset into floats with 48x48 pixel format.
images = ds.pixels.apply(lambda x : np.array(x.split(' ')).reshape(48,48,1).astype('float32'));

images = np.stack(images, axis=0);

# Gets all the emotions in text form.
labels = ds.emotion.values;

# Split Training and Testing Values
X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size=0.15);

# Split can clearly see 15% of values are testing values
X_train.shape, Y_train.shape, X_test.shape, Y_test.shape

# Create the model for training (had some help from online video tutorials)

model = keras.models.Sequential([
    keras.layers.Conv2D(32, (3,3), activation = "relu", input_shape = (48,48,1)),
    keras.layers.MaxPool2D((2,2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),

    keras.layers.Conv2D(64, (3,3), activation = "relu", input_shape = (48,48,1)),
    keras.layers.MaxPool2D((2,2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),

    keras.layers.Conv2D(64, (3,3), activation = "relu", input_shape = (48,48,1)),
    keras.layers.MaxPool2D((2,2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),

    keras.layers.Conv2D(128, (3,3), activation = "relu", input_shape = (48,48,1)),
    keras.layers.MaxPool2D((2,2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),

    keras.layers.Flatten(),

    keras.layers.Dense(128, activation = "relu"),

    # Final Output always has to be softmax
    keras.layers.Dense(7, activation="softmax")
])

model.summary();

# Comile Model and use the loss function sparse_categorical_crossentropy to reduce loss and increase accuracy.

model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss = 'sparse_categorical_crossentropy', metrics=["accuracy"])

import os

try:
  os.mkdir('checkpoint')
except:
  pass;

file_name = 'best_model';

checkpoint_path = os.path.join('checkpoint', file_name);

callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,monitor='val_accuracy',verbose=1,save_freq='epoch', save_best_only=True,save_weights_only=False,mode='max')

# Accuracy

model.fit(X_train, Y_train, epochs=100, validation_split=0.1, callbacks=callback);

# Predicting the images with model

import tensorflow as tf;

successful = 0;
total = 0;

for k in range(0, len(Y_test)):
  prediction = model.predict(tf.expand_dims(X_test[k], 0)).argmax();

  if (emotions[prediction] == emotions[Y_test[k]]):
    successful = successful + 1;
    print("Prediction Made!");


  total = total + 1;
  print(f"{successful / total * 100}% Testing Accuracy.");

# Using Custom Examples to Predict Emotion